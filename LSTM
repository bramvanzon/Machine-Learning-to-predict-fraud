import os
import numpy as np
import tensorflow as tf
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense, Input, Bidirectional, AdditiveAttention, Dropout, Permute, Reshape, Multiply
from keras.layers import BatchNormalization
from keras.regularizers import l2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, f1_score
import matplotlib.pyplot as plt
from scipy.interpolate import CubicSpline

# Function to load data from a CSV file
def load_data(file_path):
    data = pd.read_csv(file_path)
    data = data[data['gt'] != 'gt']
    data['gt'] = pd.to_numeric(data['gt'])
    data = data[(data['pump_index'] != 16 ) & (data['pump_index'] != 23) & (data['pump_index'] != 24) & (data['pump_index'] != 22)]
    return data

# Function to preprocess data
def preprocess_data(data):
    X = data.drop(columns=['date', 'symbol', 'gt'])
    y = data.groupby('pump_index')['gt'].mean()
    return X, y

# Function to reshape data for LSTM
def reshape_data(X, timesteps):
    n_instances = X['pump_index'].nunique()
    features = X.shape[1]
    X_reshaped = np.zeros((n_instances, timesteps, features))
    for i, index in enumerate(X['pump_index'].unique()):
        instance_data = X[X['pump_index'] == index]
        instance_length = min(instance_data.shape[0], timesteps)
        padded_length = min(instance_length, timesteps)
        padded_data = np.pad(instance_data.iloc[-instance_length:, :].values, ((timesteps - padded_length, 0), (0, 0)), 'constant')
        X_reshaped[i, :, :] = padded_data
    return X_reshaped


## adapted for binary and Bidirectional   
#def train_model(X_train, y_train, timesteps, features, lstm_units):
#    model = Sequential()
#    #model.add(Input(shape=(timesteps, features)))
#    model.add(Bidirectional(LSTM(lstm_units)))  
#    model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification
#    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Binary cross-entropy loss
#    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
#    return model


def train_model(X_train, y_train, timesteps, features, lstm_layers, lstm_units, l2_penalty = 0.005):
    model = Sequential()
    for i in range(lstm_layers - 1):  # Iterate over all but the last LSTM layer
        model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, kernel_regularizer=l2(l2_penalty))))  # Add LSTM layers that return sequences
    model.add(Bidirectional(LSTM(lstm_units)))  # Last LSTM layer without return_sequences
    model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Binary cross-entropy loss
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    return model


# Function to evaluate model
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    auc = roc_auc_score(y_test, predictions)
    fpr, tpr, thresholds = roc_curve(y_test, predictions)
    y_pred = np.where(predictions > 0.5, 1, 0)
    cm = confusion_matrix(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    return auc, fpr, tpr, cm, f1

# Main function
def main():
    folder_path = 'combined_folder_A/'
    file_configs = [
        #('100_last_instances_features_15S.csv', 100),
        #('100_last_instances_features_25S.csv', 100),
        #('100_last_instances_features_5S.csv', 100),
        #('150_last_instances_features_15S.csv', 150),
        #('150_last_instances_features_25S.csv', 150),
        #('150_last_instances_features_5S.csv', 150),
        #('200_last_instances_features_15S.csv', 200),
        #('200_last_instances_features_25S.csv', 200),
        #('200_last_instances_features_5S.csv', 200),
        ('50_last_instances_features_15S.csv', 50),
        #('50_last_instances_features_25S.csv', 50),
        #('50_last_instances_features_5S.csv', 50)
    ]
    lstm_configs = [
        (1, 25), (1, 50), (1, 75), (1, 100) ,(1, 125),
        (2, 25), (2, 50), (2, 75), (2, 100) ,(2, 125),
        (3, 25), (3, 50), (3, 75), (3, 100) ,(3, 125),
        (4, 25), (4, 50), (4, 75), (4, 100) ,(4, 125)
    ]
    repetitions = 15
    
    
    results = []
    for file, timesteps in file_configs:
        data = load_data(os.path.join(folder_path, file))
        X, y = preprocess_data(data)
        X_reshaped = reshape_data(X, timesteps)
        #X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y.values.reshape(-1, 1), test_size=0.2, random_state=42)
        for lstm_layers, lstm_units in lstm_configs:
            recall_per_repetition = []
            cms = []
            auc_values = []
            fprs = []
            tprs = []
            f1_scores = []
            for _ in range(repetitions):
                X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y.values.reshape(-1, 1), test_size=0.2, random_state=42)
                model = train_model(X_train, y_train, timesteps, X.shape[1], lstm_layers, lstm_units)
                auc, fpr, tpr, cm, f1 = evaluate_model(model, X_test, y_test)
                auc_values.append(auc)
                fprs.append(fpr)
                tprs.append(tpr)
                cms.append(cm)  
                f1_scores.append(f1)
                
                recall = cm[1,1] / (cm[1,0] + cm[1,1])
                recall_per_repetition.append(recall)
                
                
            avg_auc = np.mean(auc_values)
            
            min_len = min(len(fpr) for fpr in fprs)
            fprs_truncated = [fpr[:min_len] for fpr in fprs]
            tprs_truncated = [tpr[:min_len] for tpr in tprs]
            
            #thresholds_list = np.linspace(0, 1, num=100)
            #interp_fprs = [CubicSpline(thresholds_list, fpr)(thresholds_list) for fpr in fprs]
            #interp_tprs = [CubicSpline(thresholds_list, tpr)(thresholds_list) for tpr in tprs]
        
            #avg_fpr = np.mean(interp_fprs, axis=0)
            #avg_tpr = np.mean(interp_tprs, axis=0)
            avg_f1 = np.mean(f1_scores)
            avg_fpr = np.mean(fprs_truncated, axis=0)
            avg_tpr = np.mean(tprs_truncated, axis=0)
            avg_cm = np.mean(cms, axis=0)
            avg_recall = np.mean(recall_per_repetition, axis=0)
            results.append({
                'file': file,
                'timesteps': timesteps,
                'lstm_layers': lstm_layers,
                'lstm_units': lstm_units,
                'avg_auc': avg_auc,
                'avg_fpr': avg_fpr,
                'avg_tpr': avg_tpr,
                'avg_cm': avg_cm,
                'recall_per_repetition': recall_per_repetition,
                'avg_f1': avg_f1,
                'avg_recall': avg_recall
            })
                                               
    results_df = pd.DataFrame(results)
    
    if not os.path.exists("metricsA10.csv"):
        results_df.to_csv('metricsA10.csv', index=False)

    results.sort(key=lambda x: ['avg_recall'], reverse=False) 
    
           
    
    # Plot ROC curves and confusion matrices for each configuration
    for result in results:
        plt.figure(figsize=(18, 5))
        plt.suptitle(f' Configuration: {result["file"]} - LSTM Layers: {result["lstm_layers"]} - LSTM Units: {result["lstm_units"]}')
        
        # Plot ROC curve
        plt.subplot(1, 3, 1)
        plt.plot(result['avg_fpr'], result['avg_tpr'], label=f'AUC = {result["avg_auc"]:.2f}')
        plt.plot([0, 1], [0, 1], linestyle='--', color='grey')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend(loc='lower right')
        
        # Plot confusion matrix
        plt.subplot(1, 3, 2)
        cm_percentage = result['avg_cm'] / np.sum(result['avg_cm']) * 100  # Calculate percentages
        plt.imshow(cm_percentage, interpolation='nearest', cmap=plt.cm.Blues)
        plt.title('Average Confusion Matrix')
        plt.xlabel('Predicted labels')
        plt.ylabel('True labels')
        plt.xticks(np.arange(result['avg_cm'].shape[1]), ['Class 0', 'Class 1'])
        plt.yticks(np.arange(result['avg_cm'].shape[0]), ['Class 0', 'Class 1'])
        for i in range(result['avg_cm'].shape[0]):
            for j in range(result['avg_cm'].shape[1]):
                plt.text(j, i, format(cm_percentage[i, j], '.2f') + '%', horizontalalignment="center", color="black")
        plt.colorbar().remove()
        
        # Plot recall boxplot
        plt.subplot(1, 3, 3)
        plt.boxplot(result['recall_per_repetition'])
        plt.title('Recall Boxplot')
        plt.ylabel('Recall')
        plt.ylim(0, 1)   
        
        plt.savefig(f'plots_A/{result["file"]}_LSTM_Layers_{result["lstm_layers"]}_LSTM_Units_{result["lstm_units"]}.png')
        


if __name__ == "__main__":
    main()
